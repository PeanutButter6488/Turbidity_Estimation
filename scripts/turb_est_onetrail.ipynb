{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c55e06-7f3e-496e-a63b-7d5bbfed502b",
   "metadata": {},
   "source": [
    "### Turbidity Estimation using test set sampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b99865-38af-4f85-877a-a0464c968762",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import glob\n",
    "import os\n",
    "import sys \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "sys.path.insert(0, '../dip_utils')\n",
    "from matrix_utils import arr_info\n",
    "from vis_utils import (vis_pair, \n",
    "                       vis_triple)\n",
    "%run watershed.ipynb\n",
    "%run calibrate.ipynb\n",
    "%run greyscale.ipynb\n",
    "%run image_stacking.ipynb\n",
    "%run whitep_elimination.ipynb\n",
    "%run skew_correction.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "010b5c7e-b84a-4736-955e-231bb15bc76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turbidity_estimation(dest, img, theta, h, w, ref, diff_c):\n",
    "    markers = watershed(dest, img)\n",
    "\n",
    "    # Acquire the bottom and the leftmost point of the ROI\n",
    "    bottom = np.where(markers==1)[0].max()\n",
    "    left = np.where((markers!=1) & (markers!=-1))[1].min()\n",
    "    top = np.where(markers==2)[0].min()\n",
    "    right = np.where((markers!=1) & (markers!=-1))[1].max()\n",
    "\n",
    "    # 550 50 80 350\n",
    "    vert_thres = 500\n",
    "    horz_thres = 50\n",
    "    length = 80\n",
    "    dist = 330\n",
    "\n",
    "    # Avoid Extracting incorrect ROI\n",
    "    while markers[bottom-vert_thres, left+horz_thres] == 1:\n",
    "        left += horz_thres\n",
    "\n",
    "    # Corners: Top left, Top right, Bottom left, Bottom right\n",
    "    corners = []\n",
    "    tl = [left, top]\n",
    "    tr = [right, top]\n",
    "    bl = [left, bottom]\n",
    "    br = [right, bottom]\n",
    "    corners.append(tl)\n",
    "    corners.append(tr)\n",
    "    corners.append(bl)\n",
    "    corners.append(br)\n",
    "\n",
    "    dst_corners = get_destination_points(corners)\n",
    "\n",
    "    # Calibration\n",
    "    org = plt.imread(os.path.join(dest, img))\n",
    "    org = lab_trans(clear, org)\n",
    "\n",
    "    grey = to_single_channel(org)\n",
    "\n",
    "    deskewed = unwarp(dest, img, np.float32(corners), dst_corners[0])\n",
    "    dimensions = (dst_corners[1], dst_corners[2])\n",
    "    dim = (dimensions[0] / ref[0], dimensions[1] / ref[1])\n",
    "    img_dim = (int(h*dim[0]), int(w*dim[1]))\n",
    "    \n",
    "    img_resized = cv2.resize(grey, img_dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    grey = to_single_channel(img_resized)\n",
    "\n",
    "    # x_left represents the ROI of the reference, while x_right represents the ROI of the sample\n",
    "    x_left = []\n",
    "    x_right = []\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "\n",
    "    x = []\n",
    "    x1 = []\n",
    "    y = []\n",
    "    y1 = []\n",
    "    for i in range(dst_corners[1], dst_corners[1]-vert_thres, -1):\n",
    "        x.append(i)\n",
    "        y.append(np.mean(deskewed[i-1:i, horz_thres:horz_thres+length]))\n",
    "    x_left.append(x)\n",
    "    y_left.append(y)\n",
    "    for i in range(dst_corners[1], dst_corners[1]-vert_thres, -1):\n",
    "        x1.append(i)\n",
    "        y1.append(np.mean(deskewed[i-1:i, horz_thres+dist:\n",
    "                                         horz_thres+dist+length]))\n",
    "    x_right.append(x1)\n",
    "    y_right.append(y1)\n",
    "\n",
    "    x_left = np.array(x_left)\n",
    "    x_right = np.array(x_right)\n",
    "    y_left = np.array(y_left)\n",
    "    y_right = np.array(y_right)\n",
    "\n",
    "\n",
    "    num = []\n",
    "    img = img.replace('_', '.')\n",
    "    a = img.split('NTU')\n",
    "    a = [b.split('.jpg') for b in a]\n",
    "    for item in a:\n",
    "        item.remove(\"\")\n",
    "    num.append(a)\n",
    "\n",
    "    NTU = []\n",
    "    for i in num:\n",
    "        for j in i:\n",
    "            if len(j) != 0:\n",
    "                NTU.append(float(j[0]))\n",
    "\n",
    "    for i in range(x_left.shape[0]):\n",
    "        diff = abs(np.mean(abs(y_right[i] - y_left[i])))\n",
    "        diff -= diff_c\n",
    "    \n",
    "    estimated = round(diff*theta[0]+theta[1],2)\n",
    "    return estimated, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd70be-e9ff-40d4-af2d-22a69ceb240c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TE_img_list(dest, img, theta, h, w, ref, diff_c):\n",
    "    if type(img) == list:\n",
    "        estimated = []\n",
    "        NTU = []\n",
    "        for i in range(len(img)):\n",
    "            e, a = turbidity_estimation(dest, img[i], theta, h, w, ref, diff_c)\n",
    "            estimated.append(e)\n",
    "            NTU.append(a)\n",
    "        return estimated, NTU\n",
    "    else:\n",
    "        return turbidity_estimation(dest, img, theta, h, w, ref, diff_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
